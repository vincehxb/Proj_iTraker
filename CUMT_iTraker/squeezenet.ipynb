{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用SqueezeNet结构训练cumt数据\n",
    "## 2017年11月8日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %reload_ext autoreload\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from cumt_eye import Cumt_itraker\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from Cumt_SqueezeNet import squeezenet\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "cumt_data=np.load('cumt_data.pkl')\n",
    "index=np.arange(cumt_data['data'].shape[0])\n",
    "np.random.shuffle(index)\n",
    "data=cumt_data['data'][index]\n",
    "label=cumt_data['labels'][index]\n",
    "del cumt_data\n",
    "\n",
    "#cumt rgb 相片均值\n",
    "cumt_picmean=[103.939, 116.779, 123.68]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将图片从 （32,128,3）转换成（128,128,3），有可能会导致精度下降，非必要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resize_img=[]\n",
    "for i in range(data.shape[0]):\n",
    "    img=data[i]+cumt_picmean\n",
    "    img=cv2.resize(img.astype('uint8'),(128,128))\n",
    "    resize_img.append(img.astype('uint8'))\n",
    "    if i%10000==0:\n",
    "        print (i)\n",
    "data=None\n",
    "resize_img=np.array(resize_img).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 常规设置占位符，训练节点等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('PlaceHolder'):\n",
    "    X=tf.placeholder(shape=[None,128,128,3],dtype=tf.float32)\n",
    "    Y=tf.placeholder(shape=[None,10],dtype=tf.float32)\n",
    "    LR=tf.placeholder(dtype=tf.float32)\n",
    "with tf.name_scope('model'):\n",
    "    model=squeezenet(X,sess)\n",
    "    y_score=model.score\n",
    "with tf.name_scope('LOSS'):\n",
    "    LOSS=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_score,labels=Y))\n",
    "    tf.summary.scalar('loss',LOSS)\n",
    "with tf.name_scope('TRAIN'):\n",
    "    TRAIN=tf.train.AdamOptimizer(LR).minimize(LOSS)\n",
    "with tf.name_scope('ACCURACY'):\n",
    "    acc_count=tf.equal(tf.arg_max(y_score,1),tf.arg_max(Y,1))\n",
    "    ACCURACY=tf.reduce_mean(tf.cast(acc_count,tf.float32))\n",
    "    tf.summary.scalar('acc',ACCURACY)\n",
    "writer_tr=tf.summary.FileWriter('./mylog/squeezenet/train')\n",
    "writer_te=tf.summary.FileWriter('./mylog/squeezenet/test',sess.graph)\n",
    "writer_val=tf.summary.FileWriter('./mylog/squeezenet/val')\n",
    "merge=tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train,test,val数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=resize_img\n",
    "index=np.arange(data.shape[0])\n",
    "np.random.shuffle(index)\n",
    "# tr_index=index[:100]\n",
    "# te_index=index[100:200]\n",
    "tr_index=index[:int(data.shape[0]*0.9)]\n",
    "te_index=index[int(data.shape[0]*0.9):]\n",
    "tr_data,tr_label=data[tr_index],label[tr_index]\n",
    "te_data,te_label=data[te_index],label[te_index]\n",
    "del data\n",
    "del label\n",
    "del resize_img\n",
    "print(tr_data.shape,te_data.shape)\n",
    "\n",
    "\n",
    "val_d=np.load('valimg1k_1109.pkl')\n",
    "val_data=[]\n",
    "for v in range(val_d['data'].shape[0]):\n",
    "    img_=cv2.resize(val_d['data'][v],(128,128))\n",
    "    val_data.append(img_)\n",
    "val_data=np.array(val_data)\n",
    "val_label=val_d['label']\n",
    "del val_d\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.extract_network_weight('squeezenet_669.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5k each hour\n",
    "batchsize=83\n",
    "#warm up,先过拟合一个小数据集，使得网络有一个比较好的初始权值(解决网络难收敛问题)\n",
    "s_time=time.time()\n",
    "for j in range(5000):\n",
    "    mask=np.random.choice(100,batchsize,replace=False)\n",
    "    x_,y_=tr_data[mask]-cumt_picmean,tr_label[mask]\n",
    "    feed_dict={X:x_,Y:y_,LR:1e-3}\n",
    "    sess.run(TRAIN,feed_dict=feed_dict)\n",
    "    if j%10==0:\n",
    "        feed_dict={X:x_,Y:y_}\n",
    "        loss_,acc_,m_=sess.run([LOSS,ACCURACY,merge],feed_dict=feed_dict)\n",
    "        #writer_tr.add_summary(m_,i)\n",
    "        print('warm up,epoch:{},train loss:{},train accuracy:{}'.format(j,loss_,acc_))\n",
    "        if acc_>0.9:\n",
    "            print('warm up ready!time eslape:{}'.format(time.time()-s_time))\n",
    "            break\n",
    "\n",
    "#开始正式训练\n",
    "lr_=1e-3\n",
    "bestval_acc=0.7\n",
    "for i in range(1,1000*6*12):\n",
    "    mask=np.random.choice(tr_data.shape[0],batchsize,replace=False)\n",
    "    x_,y_=tr_data[mask]-cumt_picmean,tr_label[mask]\n",
    "    feed_dict={X:x_,Y:y_,LR:lr_}\n",
    "    sess.run(TRAIN,feed_dict=feed_dict)\n",
    "    if i%10==0:\n",
    "        feed_dict={X:x_,Y:y_}\n",
    "        loss_,acc_,m_=sess.run([LOSS,ACCURACY,merge],feed_dict=feed_dict)\n",
    "        writer_tr.add_summary(m_,i)\n",
    "        print('epoch:{},train loss:{},train accuracy:{}'.format(i,loss_,acc_))\n",
    "    if i%20==0:\n",
    "        mask=np.random.choice(te_data.shape[0],batchsize,replace=False)\n",
    "        x_,y_=te_data[mask]-cumt_picmean,te_label[mask]\n",
    "        feed_dict={X:x_,Y:y_}\n",
    "        loss_,acc_,m_=sess.run([LOSS,ACCURACY,merge],feed_dict=feed_dict)\n",
    "        writer_te.add_summary(m_,i)\n",
    "        print('--epoch:{},test loss:{},test accuracy:{}'.format(i,loss_,acc_))\n",
    "    if i%30==0:\n",
    "        mask=np.random.choice(val_data.shape[0],128,replace=False)\n",
    "        x_,y_=val_data[mask]-cumt_picmean,val_label[mask]\n",
    "        feed_dict={X:x_,Y:y_}\n",
    "        loss_,acc_,m_=sess.run([LOSS,ACCURACY,merge],feed_dict=feed_dict)\n",
    "        writer_val.add_summary(m_,i)\n",
    "        print('@@epoch:{},val loss:{},val accuracy:{}'.format(i,loss_,acc_))\n",
    "        if acc_>bestval_acc+0.1:\n",
    "            bestval_acc=acc_\n",
    "            model_name=r'./model_save/SqueezeNet/model_'+str(int(acc_*10000))+'.ckpt'\n",
    "            saver.save(sess,model_name,global_step=i)\n",
    "            print('!!!!!model save!!!!!')\n",
    "    if i%1000==0:\n",
    "        lr_*=0.9\n",
    "        lr_=max(lr_,1e-6)\n",
    "        print('epoch:{},learning rate change:{}'.format(i,lr_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "saver=tf.train.Saver()\n",
    "saver.save(sess,'./model_save/SqueezeNet/model_984.ckpt',global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()\n",
    "#model_984.ckpt-34925\n",
    "#model_8437.ckpt-1020\n",
    "saver.restore(sess,r'D:\\Proj_DL\\Code\\Proj_EyeTraker\\Proj_iTraker\\CUMT_iTraker\\model_save\\SqueezeNet\\model_984.ckpt-34925')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试在验证集上的平均正确率，并提取权值保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "acc_lis=[]\n",
    "for k in range(100):\n",
    "    mask=np.random.choice(val_data.shape[0],128,replace=False)\n",
    "    x_,y_=val_data[mask]-cumt_picmean,val_label[mask]\n",
    "    feed_dict={X:x_,Y:y_}\n",
    "    acc_=sess.run(ACCURACY,feed_dict=feed_dict)\n",
    "    acc_lis.append(acc_)\n",
    "    #print(acc_)\n",
    "print('acc mean:')\n",
    "np.mean(acc_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=list(tf.trainable_variables())\n",
    "model_d={}\n",
    "for k in n:\n",
    "    #变量名称\n",
    "    a=str(k).split(\"'\")[1][:-2]\n",
    "    print(a)\n",
    "    with tf.variable_scope('',reuse=True):\n",
    "        var=tf.get_variable(a)\n",
    "        model_d[a]=var.eval()\n",
    "        print(tf.shape(var).eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('squeezenet_795.pkl','wb')\n",
    "pickle.dump(obj=model_d,file=fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_lab={ \n",
    "        '7':4,'8':3,'9':2,\n",
    "        '4':7,'5':6,'6':5,\n",
    "        '1':10,'2':9,'3':8,\n",
    "'0':1}\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob=tf.nn.softmax(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=[]\n",
    "for v in range(val_d['images'].shape[0]):\n",
    "    img_=cv2.resize(val_d['images'][v],(128,128))\n",
    "    val_data.append(img_.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_=val_data-cumt_picmean\n",
    "mean_sum=0\n",
    "for k in range(50):\n",
    "    mask=np.random.choice(range(val_data.shape[0]),128,replace=False)\n",
    "    acc_=sess.run(ACCURACY,feed_dict={X:x_[mask],Y:val_d['labels'][mask]})\n",
    "    #print(acc_)\n",
    "    mean_sum+=acc_\n",
    "print('mean accuracy:')\n",
    "print(mean_sum/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getimg(addr):\n",
    "    VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "    a=imread(addr)\n",
    "    plt.imshow(a)\n",
    "    plt.show()\n",
    "    b=imresize(a-VGG_MEAN,(32,128,3)).reshape((1,32,128,3))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dir=r'D:\\Proj_DL\\Code\\Proj_EyeTraker\\eye_val\\441_5.jpg'\n",
    "b=getimg(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_=sess.run(prob,feed_dict={X:b,DROPOUT:1.,BN_FLAG:False,LR:1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(p_)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算所有可训练参数的数量\n",
    "sum_=0\n",
    "for s in list(tf.trainable_variables()):\n",
    "    s=str(s)\n",
    "    shape_=s[s.index('(')+1:s.index(')')].split(',')\n",
    "    s_=1\n",
    "    #print(shape_)\n",
    "    for i in shape_:\n",
    "        if i =='':continue\n",
    "        #print(i)\n",
    "        s_*=int(i)\n",
    "    sum_+=s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
